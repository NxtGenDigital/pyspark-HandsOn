Welcome to
      ____              __
     / __/__  ___ _____/ /__
    _\ \/ _ \/ _ `/ __/  '_/
   /__ / .__/\_,_/_/ /_/\_\   version 1.6.0
      /_/

Using Python version 2.6.6 (r266:84292, Jul 23 2015 15:22:56)
SparkContext available as sc, HiveContext available as sqlContext.
>>> sc
<pyspark.context.SparkContext object at 0x222ac50>
>>> input = [("k1", 1), ("k1", 2), ("k1", 3), ("k1", 4), ("k1", 5), 
...              ("k2", 6), ("k2", 7), ("k2", 8), 
...              ("k3", 10), ("k3", 12)]
>>> rdd = sc.parallelize(input)
>>> sumCount = rdd.combineByKey( 
...                                 (lambda x: (x, 1)), 
...                                 (lambda x, y: (x[0] + y, x[1] + 1)), 
...                                 (lambda x, y: (x[0] + y[0], x[1] + y[1])) 
...                                )
>>> sumCount.collect()
[('k3', (22, 2)), ('k2', (21, 3)), ('k1', (15, 5))]
>>> avg = sumCount.mapValues( lambda v : v[0] / v[1])
>>> avg.collect()
[('k3', 11), ('k2', 7), ('k1', 3)]
>>> 
